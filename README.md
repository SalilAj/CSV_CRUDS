# CSV_CRUDS
Generates a CSV of 10,000 people's name and age and dumps the data into MongoDB 

The aim of this project is as follows:
1) To create a program that generates a CSV file containing 10,000 unique entries of people containing their First Name, Last Name and their age.</br> 
2) Using this generated CSV file, create a program that reads this CSV file and dumps all the data into mongoDB </br>

## TASK 1</br>
This program is written in Java using maven as its dependency manager.
Refer the folder "CSVNameGenerator" for the code.
It is a simple java application that generates a CSV file of 10,000 unique names of people. </br>
</br>
Steps to Run:
1) Build a Runnable Jar file of the code
2) Run the Jar file using the command 'java -jar XXX.jar'
3) You will be prompted to input the Path of the directory where you wish to create the CSV file
as well as the number of entries you wish to have in the CSV file (1-10,000).

## TASK 2</br>
This program is written in Java using maven as its dependency manager and MongoDB as its database.
Refer the folder "CSVDBLoader" for the code.
It is a simple java application that reads the CSV file generated by the program mentioned in Task 1 and stores all the information in MongoDB </br>
</br>
Steps to Run:
1) Build a Runnable Jar file of the code
2) Run the Jar file using the command 'java -jar XXX.jar'
3) You will be prompted to input the Path of the CSV file containing th names of the people
as well as the URI of the mongoDB Database where you wish to store the information.
4) The entries will be stored in mongo database called 'genesys' and collection name called 'people'

## Note on Performance</br>
**For Task 1:**</br>
There are various ways by which you can generate 10,000 random names.</br>
However generating random names may lead to collision thereby having unique names may not be guaranteed, this becomes especially true as the number of people increase.</br>
This can be resolved using HashSet.</br>
Names generated can be stored in a Hashset and any names generated afterwards may be validated for their uniqueness by refering to the names already present in the hashset. All unique entries are further added to the hashset till the number of unique entries required are met </br>
However the downside is that you need to store 10,000 names in the hashset, more so if the program is expected to handle more than 10,000 entries. The program will also loop additional number of times depending on the number of collisions occured.</br>
The requirement of this project was to just generate unique values as names, not necessary they should be legitimate or Random names. So decision was made to go with a sequential approach to generate names to maintain uniqueness and avoid the hassel of storing them in a hashset and collision detection.

**For Task 2:**</br>
The prime performance issue in database insertion is the amount of time required for storing large number of entries.
As the number of entries increase the program will take equally longer to finish the complete insertion.</br>
Also the additional latency if the database is not local and/or is hosted on a server that is geographically distant.
One solution to reduce the time required to insert large amount of data into the database is to run the insertion operation on multiple services parallely. </br>
You can leverage the benefits of using a Master-Worker distributes system design to run the insertion operation parallely.
Assuming there are around 10 million entries, the Master can break down the 10 million entries into 10 chunks of 1 million entries and send the different chunks to 10 workers who perform the insertion operation parallely. This can greatly reduce the time required for insertion as compared to sequential insertion by a single service. 

